<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://GoodGodY.github.io">
  <title>GoodGod Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="stay simple, stay naive">
<meta property="og:type" content="website">
<meta property="og:title" content="GoodGod Blog">
<meta property="og:url" content="http://GoodGodY.github.io/index.html">
<meta property="og:site_name" content="GoodGod Blog">
<meta property="og:description" content="stay simple, stay naive">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GoodGod Blog">
<meta name="twitter:description" content="stay simple, stay naive">
  
    <link rel="alternative" href="/atom.xml" title="GoodGod Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/main.css" type="text/css">
  

</head>

<body>
  <div id="container">
    <div class="left-col">
      <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img src="null" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">GoodGod</a></h1>
		</hgroup>

		
		<p class="header-subtitle">Stay simple, stay naive</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/tags/随笔">随笔</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
	        
    		
    			
    			<a class="js-smart-menu" data-idx="0" href="javascript:void(0)">所有文章</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="1" href="javascript:void(0)">标签</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="2" href="javascript:void(0)">友链</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="3" href="javascript:void(0)">关于我</a>
    			
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="/#" title="github">github</a>
		        
					<a class="weibo" target="_blank" href="/#" title="weibo">weibo</a>
		        
					<a class="rss" target="_blank" href="/#" title="rss">rss</a>
		        
					<a class="zhihu" target="_blank" href="/#" title="zhihu">zhihu</a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"><i class="icon-list"></i></div>
  		<h1 class="header-author js-mobile-header hide">GoodGod</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				
					<img src="null" class="js-avatar">
				
			</div>
			<hgroup>
			  <h1 class="header-author">GoodGod</h1>
			</hgroup>
			
			<p class="header-subtitle">Stay simple, stay naive</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/tags/随笔">随笔</a></li>
		        
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="/#" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="/#" title="weibo">weibo</a>
			        
						<a class="rss" target="_blank" href="/#" title="rss">rss</a>
			        
						<a class="zhihu" target="_blank" href="/#" title="zhihu">zhihu</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
        
  
    <article id="post-java-optimize" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/26/java-optimize/">论jvm优化</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="一些要交代的">一些要交代的</h2><p>假设你是一个普通的 Java 对象</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/11/26/java-optimize/" class="archive-article-date">
  	<time datetime="2016-11-26T14:20:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-26</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/">java</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/内存/">内存</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-hive-mysql" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/24/hive-mysql/">科学配置hive+mysql+hadoop</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="mysql配置">mysql配置</h2><p>先前的服务器mysql有一些莫名地故障，于是准备重装。贸然重装肯定是一件愚蠢的事，首先要做的是把原有的mysql卸载干净。但如果你的mysql能正常地工作，那当我没提起这件事。卸载遵循以下步骤：</p>
<pre><code><span class="keyword">*</span>运行rpm -qa|<span class="string">grep -i mysql。 显示如下：</span>
</code></pre><p><img src="/img/3.jpg" alt=""></p>
<pre><code><span class="keyword">*</span>删掉它们，当然之前要停止mysql服务，运行rpm -e --nodeps 包名
<span class="keyword">*</span>查找之前老版本mysql的目录、并且删除老版本mysql的文件和库,运行find / -name mysql，会出现很多关于mysql的目录，删掉它们。
<span class="keyword">*</span>run <span class="string">"rm -rf /etc/my.cnf"</span>
<span class="keyword">*</span>再次查找机器是否安装mysql --&gt;rpm -qa|<span class="string">grep -i mysql</span>
</code></pre><p> 接下来安装mysql，方便起见，采用rpm的安装方式。下载mysql全家桶<a href="http://115.156.188.231/cdn.mysql.com//Downloads/MySQL-5.5/MySQL-5.5.53-1.linux2.6.x86_64.rpm-bundle.tar" target="_blank" rel="external">下载地址</a>，然后解压，安装=》sudo rpm -ivh MySQL-*。顺便转一下配置文件=》sudo cp /usr/share/mysql/my-large.cnf /etc/my.cnf。<br> 开启mysql服务=》  service mysql start。<br> 设置用户密码=》 sudo /usr/bin/mysqladmin -u root password ‘123’<br> 进入mysql=》mysql -uroot -p123<br> 创建mysql的hive用户=》 CREATE USER ‘hive’@’localhost’ IDENTIFIED BY “123”<br> 创建数据库=》create database hive;<br> 赋予权限=》 grant all on hive.<em> to hive@’%’  identified by ‘hive’; grant all on hive.</em> to hive@’localhost’  identified by ‘hive’; flush privileges;<br> 退出mysql<br>顺便可以验证下hive用户=》mysql -uhive -phive</p>
<h2 id="hadoop安装">hadoop安装</h2><p>略<br>不过注意一个问题：之前hadoop start起来没问题，也没有仔细看日志，但是运行hadoop命令总是遇到connection refused的问题，之前以为防火墙的原因。后来发现是用户bashrc的环境配置默认hadoop是另一个版本的，所以hadoop的命令工具来自另个hadoop。这个错误原因有点离谱，但还是挺难发现的。</p>
<h2 id="hive安装与配置">hive安装与配置</h2><h3 id="安装配置">安装配置</h3><p>首先下载 hive1.2.1=》 nohup wget <a href="http://mirrors.hust.edu.cn/apache/hive/stable/apache-hive-1.2.1-bin.tar.gz" target="_blank" rel="external">http://mirrors.hust.edu.cn/apache/hive/stable/apache-hive-1.2.1-bin.tar.gz</a> &amp;<br>解压 tar -zxvf ..<br>配置环境变量 .bashrc=&gt;<br>export HIVE_HOME=/home/feiwang/hive-2.0<br>export HIVE_CONF_DIR=/home/feiwang/hive-2.0/conf<br>export PATH=$PATH:$HIVE_HOME/bin<br>source .bashrc<br>修改hive配置文件，<strong>将默认文件mv成hive-site.xml，不是hive-default.xml！</strong>之前一直读取配置有问题就是这个原因。<br>主要修改以下参数：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionURL <span class="tag">&lt;/<span class="title">name</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="title">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive <span class="tag">&lt;/<span class="title">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName <span class="tag">&lt;/<span class="title">name</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="title">value</span>&gt;</span>com.mysql.jdbc.Driver <span class="tag">&lt;/<span class="title">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionPassword <span class="tag">&lt;/<span class="title">name</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="title">value</span>&gt;</span>hive <span class="tag">&lt;/<span class="title">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span> </span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="title">name</span>&gt;</span>hive.hwi.listen.port <span class="tag">&lt;/<span class="title">name</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="title">value</span>&gt;</span>9999 <span class="tag">&lt;/<span class="title">value</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="title">description</span>&gt;</span>This is the port the Hive Web Interface will listen on <span class="tag">&lt;/<span class="title">descript</span> <span class="attribute">ion</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>Username to use against metastore database<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>thrift://127.0.0.1:9083<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>hive.exec.scratchdir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/tmp/hive<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: $&#123;hive.exec.scratchdir&#125;/&amp;lt;username&amp;gt; is created, with $&#123;hive.scratch.dir.permission&#125;.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>hive.exec.local.scratchdir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/tmp/hive/local<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>Local scratch space for Hive jobs<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>hive.downloaded.resources.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/tmp/hive/resources<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">description</span>&gt;</span>Temporary local directory for added resources in the remote file system.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>下载jdbc的jar包=》wget <a href="http://115.156.188.231/cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.40.tar.gz" target="_blank" rel="external">http://115.156.188.231/cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.40.tar.gz</a></p>
<p>拷贝mysql-connector-java-5.1.40-bin.jar 到hive 的lib下面<br>mv mysql-connector-java-5.1.40-bin.jar /root/feiwang/hive-2.0/lib/</p>
<p>启动元数据 =》<strong>bin/hive –service metastore &amp;</strong></p>
<h3 id="验证">验证</h3><p>运行 hive命令,创建表：<br><img src="/img/4.jpg" alt=""><br>进入mysql会发现hive数据库自动生成了很多表<br><img src="/img/5.jpg" alt=""><br>进入hadoop的hdfs会发现也有两个文件夹生成了<br>Found 2 items<br><strong>drwxr-xr-x   - root supergroup          0 2016-11-24 21:43 /user/hive/warehouse/diablo</strong><br><strong>drwxr-xr-x   - root supergroup          0 2016-11-24 19:45 /user/hive/warehouse/kzx</strong></p>
<h3 id="简单介绍下hive">简单介绍下hive</h3><p>Hive是运行在Hadoop之上的数据仓库，将结构化的数据文件映射为一张数据库表，提供简单类SQL查询语言，称为HQL，并将SQL语句转换成MapReduce任务运。有利于利用SQL语言查询、分析数据，适于处理不频繁变动的数据。Hive底层可以是HBase或者HDFS存储的文件。两者都是基于Hadoop上不同的技术，相互结合使用，可处理企业中不同类型的业务，利用Hive处理非结构化离线分析统计，利用HBase处理在线查询。<br>Hive三种元数据存储方式：<br>1&gt;.本地derby存储，只允许一个用户连接Hive，适用于测试环境<br>2&gt;.本地/远程MySQL存储，支持多用户连接Hive，适用于生产环境</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/11/24/hive-mysql/" class="archive-article-date">
  	<time datetime="2016-11-24T07:39:47.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-24</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hive/">hive</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mysql/">mysql</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/配环境/">配环境</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-stephen-chow" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/11/stephen-chow/">什么样的Stephen Chow</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>我一直认为,在我进入大学后出现了两位我人生中的精神导师,这个词意义说起来挺重大的,但我的确是认真<br>的说出了这句话.他们一位是作家,一位是演员.他们的名字叫做王小波和周星驰.<br>//待更新,看心情</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/11/11/stephen-chow/" class="archive-article-date">
  	<time datetime="2016-11-11T14:20:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-11-11</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/周星驰/">周星驰</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/扯淡学/">扯淡学</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-diablo" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/10/diablo/">在大菠萝中遇到的一些小问题</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="大菠萝的任务">大菠萝的任务</h2><p>大菠萝全名diablo technology,是一家硬件技术外企.但是我是一名纯软界的小小码农,怎么会帮一个硬件公司做事呢.起初是这家公司中国区的技术人员跟我们组里的陆博大大比较熟,他们想招一两个懂Spark的人帮他们做一些测试和性能上的优化来得出他们研发的新硬件M1相比于市场上普通内存的特性与性能差异.于是陆博建议我担任这份兼职,工作不多而且还可以增一份不错的收入,众所周知,这种事情我是无法拒绝的.</p>
<p>为了推广M1,他们需要在M1上运行很多工业市场上流行的吃内存的软件与平台,包括spark,mysql,redis等.所以这三个月我就没事帮他们写一点测试的代码,优化spark的配置,寻找运行错误的bug,搭建redis-cluster这种活.期间爬过很多坑,有几个瞬间我感觉到自己是技术顾问,让我小小地膨胀一下.</p>
<h2 id="C程序的测试">C程序的测试</h2><p>C语言我是有好几年没碰了,作为一个大四以后一直在jvm语言中游走,有时用python写点小项目的人,几乎对指针这种东西处于懵逼模式.</p>
<p>代码的编写和测试流程我就不详细说了,说说遇到的段错误解决方案.也是这次跑C程序让我有了了解这方面知识的契机.发生了段错误/core dump,总结一下原因,一般是这几个:</p>
<ul>
<li>访问了不存在的内存地址</li>
<li>访问了系统保护的内存地址</li>
<li>访问了只读的内存地址等等情况</li>
</ul>
<p><strong>先用gdb -g -o xx xx.c 生成gdb可调试的文件。</strong>说到gdb调试,大概要知道这几个命令:</p>
<ol>
<li><p>gdb xx 进入gdb调试界面</p>
</li>
<li><p>b 行数 在第多少行打断点</p>
</li>
<li><p>按r进入运行状态</p>
</li>
<li><p>n是单步调试</p>
</li>
<li><p>s进入函数中调试</p>
</li>
<li><p>q是退出</p>
</li>
</ol>
<p>继续说解决core dump的bug, 然后运行程序, ./xx发生段错误后会生成core文件.接着运行命令<strong>gdb xx core.36129</strong>.输入where 会打印出具体的core dump发生在代码中哪个位置，方便定位bug。</p>
<p>还有一个bug是,每次都在内存地址被free的时候报无效指针,这种情况一般包含以下几个原因:</p>
<ol>
<li><p>一个地址被free了两次, 当然这个很容易查出来.</p>
</li>
<li><p>free的地址已经不是当时申请内存的起始地址了,注意可能在程序中的一些函数中对其做了一些操作.</p>
</li>
</ol>
<h2 id="Spark-Sql测试框架中踩得坑">Spark-Sql测试框架中踩得坑</h2><p>大菠萝公司的喵叽(算我们的上司大人)想在机器上测试Spark Sql在M1上跑的性能，让我调研下Spark-sql-perf这个已有的测试框架<a href="https://github.com/databricks/spark-sql-perf" target="_blank" rel="external">(项目地址)</a>。看了一下框架代码，差不多就是写了很多sql语句封装成一个个benchmark对象然后有一个多线程模式，然后一起开测。首先，在大菠萝的机器上部署测试框架，然后写一个简单的测试程序。代码如下:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">object SqlTest extends Serializable&#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val sparkConf  = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"test"</span>)</span><br><span class="line">    .setMaster(<span class="string">"local"</span>)</span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(sparkConf)</span><br><span class="line">    val sqlContext = <span class="keyword">new</span> org.apache.spark.sql.SQLContext(sc)</span><br><span class="line">   val tables = <span class="keyword">new</span> Tables(sqlContext, <span class="string">"/Users/iceke/projects/tpcds-kit/tools"</span>, </span><br><span class="line">   Integer.parseInt(<span class="string">"1"</span>))</span><br><span class="line">    tables.genData(<span class="string">"data"</span>,<span class="string">"parquet"</span>,<span class="keyword">true</span>, <span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">false</span>)</span><br><span class="line">    val tableNames = Array(<span class="string">"call_center"</span>, <span class="string">"catalog_page"</span>,  <span class="string">"catalog_returns"</span>, <span class="string">"catalog_sales"</span>,</span><br><span class="line">      <span class="string">"customer"</span>, <span class="string">"customer_address"</span>,  <span class="string">"customer_demographics"</span>, <span class="string">"date_dim"</span>,</span><br><span class="line">      <span class="string">"household_demographics"</span>, <span class="string">"income_band"</span>,   <span class="string">"inventory"</span>, <span class="string">"item"</span>, <span class="string">"promotion"</span>,</span><br><span class="line">      <span class="string">"reason"</span>, <span class="string">"ship_mode"</span>,  <span class="string">"store"</span>, <span class="string">"store_returns"</span>,  <span class="string">"store_sales"</span>, <span class="string">"time_dim"</span>,</span><br><span class="line">      <span class="string">"warehouse"</span>, <span class="string">"web_page"</span>,   <span class="string">"web_returns"</span>, <span class="string">"web_sales"</span>, <span class="string">"web_site"</span>)</span><br><span class="line">    <span class="keyword">for</span>(i &lt;- <span class="number">0</span> to tableNames.length - <span class="number">1</span>) &#123;</span><br><span class="line">      val a = sqlContext.read.parquet(<span class="string">"data"</span> + <span class="string">"/"</span> + tableNames&#123;i&#125;)</span><br><span class="line">     <span class="comment">// sc.broadcast(a)</span></span><br><span class="line">      a.registerTempTable(tableNames&#123;i&#125;)</span><br><span class="line">    &#125;</span><br><span class="line">    val tpcds = <span class="keyword">new</span> TPCDS (sqlContext = sqlContext)</span><br><span class="line">    val experiment = tpcds.runExperiment(tpcds.tpcds1_4Queries, iterations = <span class="number">1</span>,forkThread=<span class="keyword">false</span>)</span><br><span class="line">    experiment.waitForFinish(<span class="number">60</span>*<span class="number">60</span>*<span class="number">10</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<p>先生成数据，大概几十张表。然后加载这几十张表，进行benchmark实验。本来过程很简单，但是需求是无止境的，喵叽说试试把所有的表cache到内存中，这样进行查询时快一点，听上去非常简单，在注册后加上一行cache表的代码就可以了。部分代码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> val a = sqlContext.read.parquet(<span class="string">"data"</span> + <span class="string">"/"</span> + tableNames&#123;i&#125;)</span><br><span class="line"><span class="comment">// sc.broadcast(a)</span></span><br><span class="line"> a.registerTempTable(tableNames&#123;i&#125;)</span><br><span class="line"> sqlContext.cacheTable(tableNames&#123;i&#125;)</span><br></pre></td></tr></table></figure></p>
<p>我在mac上用local模式测了一下ok便提交给喵叽。</p>
<p>事情永远不会像想象的那么顺利，喵叽在集群上一测，就通过了几个benchmark，然后大量的stage报错，全部都是failed to get broadcast(TorrentBroadcast)异常，然后stage 直接失败，查看executor的日志发现已经有的broacast被remove了，但是接下来的加个task又会去获取这些broadcast，便会直接失败。这明显是不符合逻辑的，明明cache了所有表，靠异常堆栈信息并不好定位到错误的地方。我看了spark主页发现storage页面cache的RDD过一段时间就会消失，这也是一个重要的线索。</p>
<p>测试了半天还是出错，我开始静下心慢慢跟着框架的代码走，我发现最后每一个线程都会执行一个doBenchmark方法，<br>方法里面最后执行完查询操作之后会做一些善后处理。相关代码：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> def <span class="title">benchmark</span><span class="params">(</span><br><span class="line">    includeBreakdown: Boolean,</span><br><span class="line">    description: String = <span class="string">""</span>,</span><br><span class="line">    messages: ArrayBuffer[String],</span><br><span class="line">    timeout: Long,</span><br><span class="line">    forkThread: Boolean = <span class="keyword">true</span>)</span>: BenchmarkResult </span>= &#123;</span><br><span class="line">  logger.info(s<span class="string">"$this: benchmark"</span>)</span><br><span class="line">  sparkContext.setJobDescription(s<span class="string">"Execution: $name, $description"</span>)</span><br><span class="line">  beforeBenchmark()</span><br><span class="line">  val result = <span class="keyword">if</span> (forkThread) &#123;</span><br><span class="line">    runBenchmarkForked(includeBreakdown, description, messages, timeout)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    doBenchmark(includeBreakdown, description, messages)</span><br><span class="line">  &#125;</span><br><span class="line">  println(<span class="string">"!!!!!!!!!!!!!!!!after Bench"</span>)</span><br><span class="line">  afterBenchmark(sqlContext.sparkContext)</span><br><span class="line">  result</span><br></pre></td></tr></table></figure></p>
<p>这个善后处理是afterBenchmark方法，方法代码：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">afterBenchmark</span><span class="params">(sc: SparkContext)</span>: Unit </span>= &#123;</span><br><span class="line">  <span class="comment">// Best-effort clean up of weakly referenced RDDs, shuffles, and broadcasts</span></span><br><span class="line">  System.gc()</span><br><span class="line">  <span class="comment">// Remove any leftover blocks that still exist</span></span><br><span class="line">  sc.getExecutorStorageStatus</span><br><span class="line">      .flatMap &#123; status =&gt; status.blocks.map &#123; <span class="keyword">case</span> (bid, _) =&gt; bid &#125; &#125;</span><br><span class="line">      .foreach &#123; bid =&gt; SparkEnv.get.blockManager.master.removeBlock(bid) &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>它会对每个相关block的id进行移除，无论它是否做了cache。所以解决方法是注释掉afterBenchmark方法，与上次堆外内存的bug一样，发现bug的过程是痛苦的，解决方案是简单到发指的,让人痛苦,又让人快乐.</p>
<p>所以说，别人的框架不是万能的，虽然你操作起来更便捷，但你必须遵守它制定的那一些规则，有些规则是操蛋的，当你使用它时，如果一直被操蛋的bug所围困，就需要看看它的源码看看是否有问题，或者与你的策略存在偶然性的冲突。</p>
<p><strong>最后, 我的梦想是成为规则的制定者.</strong></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/10/10/diablo/" class="archive-article-date">
  	<time datetime="2016-10-10T07:20:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-10-10</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/C语言/">C语言</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/bug/">bug</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/内存/">内存</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-spark-bug" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/28/spark-bug/">修改Spark内核(Deca)所产生的两个奇怪bug</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Spark将数据放在jvm堆外导致无故卡死">Spark将数据放在jvm堆外导致无故卡死</h2><h3 id="思路实现与问题产生">思路实现与问题产生</h3><p>咱们内存计算小组陆博的文章Deca经过一年的努力终于中了vldb,作为其中参与了一些微小的工作的男人,虽然对其中一部分细节不是太清楚,因为这个Deca系统的开发涉及到将近5个人,我的工作囊括一下就是: </p>
<pre><code><span class="number">1.</span>完成了其中UDF的转换,将方法的操作转为对字节数组的操作;
<span class="number">2.</span>手写了Deca的手动版的代码,就是利用Deca的思想对Spark应用的代码进行改造;
<span class="number">3.</span>进行了大量的测试并统计GC时间,stage时间等相关数据.
</code></pre><p>其实Deca系统的核心思想就是将原有的java大对象转化为字节数组有序地放置在jvm中,这样一可以减少对内存的使用,<br>也可以基本避免所有的GC.附上论文<a href="https://arxiv.org/pdf/1602.01959v3.pdf" target="_blank" rel="external">Deca论文地址</a></p>
<p>老板的top中了之后,当然会将它扩展扩展然后投期刊,这是基本套路.要求扩展30%,其中就包括将之前的手动版的数据放置在堆外,还是以字节数组的形式来和堆内版本进行比较,理论上来说堆外版本肯定性能是比堆内好的,毕竟放置在堆外可以完全逃避GC的控制,也更加符合Deca的思想.</p>
<p>代码实现并不难,基本由Unsafe这个类操作完成.大概思路就是将Spark应用中需要缓存的RDD其中的partition的对象用字节数组的形式写在堆外,读的时候再直接按照偏移量读取,贴上部<br>分代码:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> UnsafePR._</span><br><span class="line"><span class="keyword">private</span> val baseAddress = UNSAFE.allocateMemory(size)</span><br><span class="line"><span class="keyword">private</span> var curAddress = baseAddress</span><br><span class="line">def address = baseAddress</span><br><span class="line">def free:Unit=&#123;</span><br><span class="line">  UNSAFE.freeMemory(baseAddress)</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">def <span class="title">writeInt</span><span class="params">(num:Int)</span>:Unit</span>=&#123;</span><br><span class="line">  UNSAFE.putInt(curAddress,num)</span><br><span class="line">  curAddress += <span class="number">4</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><a href="https://github.com/zx247549135/DecaApplication/blob/master/src/main/scala-2.10/decaApp/UnsafePR.scala" target="_blank" rel="external">完整的Deca手动版PageRank代码地址</a></p>
<p>起先在本机的local模式测试了堆外版LR和PageRank应用当然是没问题的,结果也是正确的.然后转移到服务器集群上进行测试.令人惊喜的是,一个神奇的bug出现了.</p>
<h3 id="Bug的特征">Bug的特征</h3><p>此Bug是本人coding以来见识到的算的上奇怪的一个bug了,它有以下几个特征.首先LR的堆外版本集群测试是没有问题的,但是切换到PageRank的堆外版本来测试时,总是在ZipPartition这个stage最后几个task执行的时候jvm crash掉,这个job就直接卡死了,必须手动杀掉才能停止.executor异常日志:<br><img src="/img/1.jpg" alt="executor异常日志"></p>
<p>stage卡住图示:<br><img src="/img/2.jpg" alt="stage卡住"></p>
<p>而且还有一个特殊的症状:就是PR跑2G数据量的时候居然不会挂掉,一到7G和20G的时候就会挂掉.而且local模式不会出错,一个executor也不会出错,一旦增加到多个executor就会出错.</p>
<h3 id="Bug原因分析与结论">Bug原因分析与结论</h3><p>一开始想到的原因是shuffle的问题,因为local和单个executor不会出错,一旦涉及到网络传输就会报错.我怀疑是不是序列化方式的问题,分别用kryo和java自带的序列化方式测试了一下,然而都会报错.后来想了想应该不是网络传输的问题,不然小数据量怎么可以通过.我上网查了一下jvm crash那段报错信息,基本都是由于Unsafe访问到非法位置的原因,于是开始往这个方向考虑.</p>
<p>最后与师兄讨论中意识到问题的关键所在,首先Spark一个executor执行task比较慢时,如果另一个executor执行完一个stage的所有task时,会将剩余的task调度到那个节点去执行,也就是non-local task.non-local task是从网络传输过去的,这部分task是由cache RDD的partition生成而来的,这部分task是从block manager过去的,然而partition中的UnsafeEdge对象中只有一个Unsafe成员变量,一个初始地址和终点地址,和分配在jvm堆内的对象不同,并不携带真正的数据.所以这个task被调度到其他executor时,自然会非法访问堆外内存,然后jvm crash掉,这也可以解释为什么stage中位置为Any的task都不能成功执行这个现象.至于2G的数据量为什么可以通过,因为task运行的时间很短,几乎不需要调度就可以在一个executor中全部完成.</p>
<p>所以解决方案就是尽量不让task调度到其他的executor上执行,可以尽量增大spark.locality.wait这个变量来避免出错.</p>
<h2 id="硕大的cache数据">硕大的cache数据</h2><pre><code>这个就简洁地介绍一下了,这是我大概<span class="number">11</span>月<span class="number">10</span>号遇见的
</code></pre><h3 id="背景">背景</h3><p>关于shuffle的VST拆解部分需要加到论文修改中,不过在意外中我发现了之前Deca release1.0版本的一个bug.那就是由于Deca将cache数据的对象完全转化为字节数组存储在jvm中,但是吊诡的现象在于Deca cache的数据居然比原生Spark的还大,这明显是不合理的.初看了一下代码发现是没有问题的.于是联系已经毕业的裴师兄,他说他之前就遇到这个问题了,只不过一直没改.这应该就是传说中的前人挖坑,后人填坑.但我同时也是很兴奋的,因为我们之前手动版本的实验结果很合理,改动Spark内核的自动版本也是同样地思路和流程,但是却出现了这种奇怪的现象,你知道解决bug是一个很能产生成就感的一个举措.在SparkContext将cache的RDD的迭代器做了一次调整,生成一个新的RDD并cache,然后将原本cache的RDD释放掉,重新调整一下RDD链,这样缓存的RDD将会被我们生成的RDD替换掉.之前对缓存的RDD所做的操作是:将里面返回KV对的迭代器改写一下,变换成往一个字节数组缓冲区写字节数组(按顺序写),然后返回新的迭代器.</p>
<h3 id="修复">修复</h3><p>后来发现之前返回的迭代器基本单位还是一个个KV对,这样就算是按字节数组写了也还是和Spark Cache的数据占用着差不多的大小,于是很简单啦,这里我将一块字节数组区称为CacheChunk,事实上它的类名也是这个.生成CacheChunk后,用Iterator包装一下便返回.跑了一下local模式,结果可想而知,抛出血红色的异常,我用的Idea,不知道其他的编辑器是不是这样.众所周知,Spark的每一个stage的结束要么是ShuffleTask,要么是ResultTask.ShuffleTask需要落磁盘,往block写点什么,这时候出发真正的RDD计算,就是调用RDD的迭代器.当然这里有个判断,如果某个RDD被定义了persist,第一次计算时会将它的计算结果常驻在内存中再返回迭代器,这样下一个stage用到此RDD时便直接在内存中获取,无需计算.管理这个流程的是一个叫CacheManager的哥们,我们之前返回的迭代器是一个iterator,iterator里包含着CacheChunk,CacheChunk里面又有一个迭代器方法,所以CacheManager肯定识别不了了呗.方便,给这个变换的CacheRDD加一个标签变量,切勿不要给它trasient这个标识,它需要被序列化.如果是Deca的RDD便让CacheManager多一个步骤,获取迭代器之后,强制转换为CacheChunk,再取一次迭代器,这样就能获取真正的数据.好,测试通过,perfect.自信地放到集群中去测试,令人”欣慰”的是,cache的数据反而更巨大!</p>
<p>后来怎么办呢,研究CacheManager下面的代码,继续调试往里面跟着走,我只想感叹一句,真的很深…就连一个普通的HashMap Spark都会根据自己的需求改.最后发现RDD的大小的评估是一个SizeEstimator的类实现的,部分功能代码如下:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">visitSingleObject</span><span class="params">(obj: AnyRef, state: SearchState)</span> </span>&#123;</span><br><span class="line">  val cls = obj.<span class="function">getClass</span><br><span class="line">  <span class="title">if</span> <span class="params">(cls.isArray)</span> </span>&#123;</span><br><span class="line">    visitArray(obj, cls, state)</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (obj.isInstanceOf[ClassLoader] || obj.isInstanceOf[Class[_]]) &#123;</span><br><span class="line">    <span class="comment">// Hadoop JobConfs created in the interpreter have a ClassLoader, which greatly confuses</span></span><br><span class="line">    <span class="comment">// the size estimator since it references the whole REPL. Do nothing in this case. In</span></span><br><span class="line">    <span class="comment">// general all ClassLoaders and Classes will be shared between objects anyway.</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    val classInfo = getClassInfo(cls)</span><br><span class="line">    state.size += alignSize(classInfo.shellSize)</span><br><span class="line">    <span class="keyword">for</span> (field &lt;- classInfo.pointerFields) &#123;</span><br><span class="line">      state.enqueue(field.get(obj))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>大概思路就是判断对象类型,原生类型直接算,数组就累加算.如果不是原生类型,继续将里面的成员变量入栈,递归调用此函数.当我调试到这儿看调试信息的时候,发现一共访问了有一千多个成员变量,明显不合理.后来发现,CacheChunk里有个Spark定制的IterupptedIterator,这里面带出一批Spark相关的变量,导致评估大小大了整整十几倍.可是CacheChunk是可以不需要这个iterator的,于是将它换了个地方.重新测试,终于okay.流下了喜悦的眼泪,想高歌一曲,想起在实验室便作罢.</p>
<p>之后还遇到CacheChunk自动扩增容量的问题,不过解决起来比较简单,就不在此描述.</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/09/28/spark-bug/" class="archive-article-date">
  	<time datetime="2016-09-28T07:39:47.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-09-28</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/bug/">bug</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  


      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2016 GoodGod
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: false,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		root: "/",
		innerArchive: true
	}
</script>

<script src="/./main.js" type="text/javascript"></script>


    
<div class="tools-col">
  <ul class="btn-wrap">
    
      <li class="chose" data-hook="tools-section-all"><span class="text">全部</span><i class="icon-book"></i></li>
    
    
      <li data-hook="tools-section-tag"><span class="text">标签</span><i class="icon-price-tags"></i></li>
    
    
      <li data-hook="tools-section-friends"><span class="text">友链</span><i class="icon-link"></i></li>
    
    
      <li data-hook="tools-section-me"><span class="text">我</span><i class="icon-smile"></i></li>
    
  </ul>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all chose">
    	</section>
    

    
    	<section class="tools-section tools-section-tag">
    			<div class="widget tagcloud" id="js-tagcloud">
    				<a href="/tags/C语言/" style="font-size: 10px;">C语言</a> <a href="/tags/NIO/" style="font-size: 10px;">NIO</a> <a href="/tags/bug/" style="font-size: 20px;">bug</a> <a href="/tags/hive/" style="font-size: 10px;">hive</a> <a href="/tags/java/" style="font-size: 20px;">java</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/photo/" style="font-size: 10px;">photo</a> <a href="/tags/spark/" style="font-size: 10px;">spark</a> <a href="/tags/内存/" style="font-size: 20px;">内存</a> <a href="/tags/周星驰/" style="font-size: 10px;">周星驰</a> <a href="/tags/天池/" style="font-size: 10px;">天池</a> <a href="/tags/扯淡学/" style="font-size: 20px;">扯淡学</a> <a href="/tags/摇滚/" style="font-size: 10px;">摇滚</a> <a href="/tags/配环境/" style="font-size: 10px;">配环境</a>
    			</div>
    	</section>
    

    
    	<section class="tools-section tools-section-friends">
  		
  			<div class="friends-wrap" id="js-friends">
  			
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接1</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接2</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接3</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接4</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接5</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接6</a>
  	        
  	        </div>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">很惭愧&lt;br&gt;&lt;br&gt;只做了一点微小的工作&lt;br&gt;谢谢大家</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>