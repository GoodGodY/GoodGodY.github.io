<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[GoodGod Blog]]></title>
  <subtitle><![CDATA[I have nothing,I have everything]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://GoodGodY.github.io/"/>
  <updated>2016-11-26T15:38:44.000Z</updated>
  <id>http://GoodGodY.github.io/</id>
  
  <author>
    <name><![CDATA[GoodGod]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[在大菠萝中遇到的一些小问题]]></title>
    <link href="http://GoodGodY.github.io/2016/10/10/diablo/"/>
    <id>http://GoodGodY.github.io/2016/10/10/diablo/</id>
    <published>2016-10-10T07:20:00.000Z</published>
    <updated>2016-11-26T15:38:44.000Z</updated>
    <content type="html"><![CDATA[<h2 id="大菠萝的任务">大菠萝的任务</h2><p>大菠萝全名diablo technology,是一家硬件技术外企.但是我是一名纯软界的小小码农,怎么会帮一个硬件公司做事呢.起初是这家公司中国区的技术人员跟我们组里的陆博大大比较熟,他们想招一两个懂Spark的人帮他们做一些测试和性能上的优化来得出他们研发的新硬件M1相比于市场上普通内存的特性与性能差异.于是陆博建议我担任这份兼职,工作不多而且还可以增一份不错的收入,众所周知,这种事情我是无法拒绝的.</p>
<p>为了推广M1,他们需要在M1上运行很多工业市场上流行的吃内存的软件与平台,包括spark,mysql,redis等.所以这三个月我就没事帮他们写一点测试的代码,优化spark的配置,寻找运行错误的bug,搭建redis-cluster这种活.期间爬过很多坑,有几个瞬间我感觉到自己是技术顾问,让我小小地膨胀一下.</p>
<h2 id="C程序的测试">C程序的测试</h2><p>C语言我是有好几年没碰了,作为一个大四以后一直在jvm语言中游走,有时用python写点小项目的人,几乎对指针这种东西处于懵逼模式.</p>
<p>代码的编写和测试流程我就不详细说了,说说遇到的段错误解决方案.也是这次跑C程序让我有了了解这方面知识的契机.发生了段错误/core dump,总结一下原因,一般是这几个:</p>
<ul>
<li>访问了不存在的内存地址</li>
<li>访问了系统保护的内存地址</li>
<li>访问了只读的内存地址等等情况</li>
</ul>
<p><strong>先用gdb -g -o xx xx.c 生成gdb可调试的文件。</strong>说到gdb调试,大概要知道这几个命令:</p>
<ol>
<li><p>gdb xx 进入gdb调试界面</p>
</li>
<li><p>b 行数 在第多少行打断点</p>
</li>
<li><p>按r进入运行状态</p>
</li>
<li><p>n是单步调试</p>
</li>
<li><p>s进入函数中调试</p>
</li>
<li><p>q是退出</p>
</li>
</ol>
<p>继续说解决core dump的bug, 然后运行程序, ./xx发生段错误后会生成core文件.接着运行命令<strong>gdb xx core.36129</strong>.输入where 会打印出具体的core dump发生在代码中哪个位置，方便定位bug。</p>
<p>还有一个bug是,每次都在内存地址被free的时候报无效指针,这种情况一般包含以下几个原因:</p>
<ol>
<li><p>一个地址被free了两次, 当然这个很容易查出来.</p>
</li>
<li><p>free的地址已经不是当时申请内存的起始地址了,注意可能在程序中的一些函数中对其做了一些操作.</p>
</li>
</ol>
<h2 id="Spark-Sql测试框架中踩得坑">Spark-Sql测试框架中踩得坑</h2><p>大菠萝公司的喵叽(算我们的上司大人)想在机器上测试Spark Sql在M1上跑的性能，让我调研下Spark-sql-perf这个已有的测试框架<a href="https://github.com/databricks/spark-sql-perf" target="_blank" rel="external">(项目地址)</a>。看了一下框架代码，差不多就是写了很多sql语句封装成一个个benchmark对象然后有一个多线程模式，然后一起开测。首先，在大菠萝的机器上部署测试框架，然后写一个简单的测试程序。代码如下:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">object SqlTest extends Serializable&#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val sparkConf  = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"test"</span>)</span><br><span class="line">    .setMaster(<span class="string">"local"</span>)</span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(sparkConf)</span><br><span class="line">    val sqlContext = <span class="keyword">new</span> org.apache.spark.sql.SQLContext(sc)</span><br><span class="line">   val tables = <span class="keyword">new</span> Tables(sqlContext, <span class="string">"/Users/iceke/projects/tpcds-kit/tools"</span>, </span><br><span class="line">   Integer.parseInt(<span class="string">"1"</span>))</span><br><span class="line">    tables.genData(<span class="string">"data"</span>,<span class="string">"parquet"</span>,<span class="keyword">true</span>, <span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">false</span>)</span><br><span class="line">    val tableNames = Array(<span class="string">"call_center"</span>, <span class="string">"catalog_page"</span>,  <span class="string">"catalog_returns"</span>, <span class="string">"catalog_sales"</span>,</span><br><span class="line">      <span class="string">"customer"</span>, <span class="string">"customer_address"</span>,  <span class="string">"customer_demographics"</span>, <span class="string">"date_dim"</span>,</span><br><span class="line">      <span class="string">"household_demographics"</span>, <span class="string">"income_band"</span>,   <span class="string">"inventory"</span>, <span class="string">"item"</span>, <span class="string">"promotion"</span>,</span><br><span class="line">      <span class="string">"reason"</span>, <span class="string">"ship_mode"</span>,  <span class="string">"store"</span>, <span class="string">"store_returns"</span>,  <span class="string">"store_sales"</span>, <span class="string">"time_dim"</span>,</span><br><span class="line">      <span class="string">"warehouse"</span>, <span class="string">"web_page"</span>,   <span class="string">"web_returns"</span>, <span class="string">"web_sales"</span>, <span class="string">"web_site"</span>)</span><br><span class="line">    <span class="keyword">for</span>(i &lt;- <span class="number">0</span> to tableNames.length - <span class="number">1</span>) &#123;</span><br><span class="line">      val a = sqlContext.read.parquet(<span class="string">"data"</span> + <span class="string">"/"</span> + tableNames&#123;i&#125;)</span><br><span class="line">     <span class="comment">// sc.broadcast(a)</span></span><br><span class="line">      a.registerTempTable(tableNames&#123;i&#125;)</span><br><span class="line">    &#125;</span><br><span class="line">    val tpcds = <span class="keyword">new</span> TPCDS (sqlContext = sqlContext)</span><br><span class="line">    val experiment = tpcds.runExperiment(tpcds.tpcds1_4Queries, iterations = <span class="number">1</span>,forkThread=<span class="keyword">false</span>)</span><br><span class="line">    experiment.waitForFinish(<span class="number">60</span>*<span class="number">60</span>*<span class="number">10</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<p>先生成数据，大概几十张表。然后加载这几十张表，进行benchmark实验。本来过程很简单，但是需求是无止境的，喵叽说试试把所有的表cache到内存中，这样进行查询时快一点，听上去非常简单，在注册后加上一行cache表的代码就可以了。部分代码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> val a = sqlContext.read.parquet(<span class="string">"data"</span> + <span class="string">"/"</span> + tableNames&#123;i&#125;)</span><br><span class="line"><span class="comment">// sc.broadcast(a)</span></span><br><span class="line"> a.registerTempTable(tableNames&#123;i&#125;)</span><br><span class="line"> sqlContext.cacheTable(tableNames&#123;i&#125;)</span><br></pre></td></tr></table></figure></p>
<p>我在mac上用local模式测了一下ok便提交给喵叽。</p>
<p>事情永远不会像想象的那么顺利，喵叽在集群上一测，就通过了几个benchmark，然后大量的stage报错，全部都是failed to get broadcast(TorrentBroadcast)异常，然后stage 直接失败，查看executor的日志发现已经有的broacast被remove了，但是接下来的加个task又会去获取这些broadcast，便会直接失败。这明显是不符合逻辑的，明明cache了所有表，靠异常堆栈信息并不好定位到错误的地方。我看了spark主页发现storage页面cache的RDD过一段时间就会消失，这也是一个重要的线索。</p>
<p>测试了半天还是出错，我开始静下心慢慢跟着框架的代码走，我发现最后每一个线程都会执行一个doBenchmark方法，<br>方法里面最后执行完查询操作之后会做一些善后处理。相关代码：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">final</span> def <span class="title">benchmark</span><span class="params">(</span><br><span class="line">    includeBreakdown: Boolean,</span><br><span class="line">    description: String = <span class="string">""</span>,</span><br><span class="line">    messages: ArrayBuffer[String],</span><br><span class="line">    timeout: Long,</span><br><span class="line">    forkThread: Boolean = <span class="keyword">true</span>)</span>: BenchmarkResult </span>= &#123;</span><br><span class="line">  logger.info(s<span class="string">"$this: benchmark"</span>)</span><br><span class="line">  sparkContext.setJobDescription(s<span class="string">"Execution: $name, $description"</span>)</span><br><span class="line">  beforeBenchmark()</span><br><span class="line">  val result = <span class="keyword">if</span> (forkThread) &#123;</span><br><span class="line">    runBenchmarkForked(includeBreakdown, description, messages, timeout)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    doBenchmark(includeBreakdown, description, messages)</span><br><span class="line">  &#125;</span><br><span class="line">  println(<span class="string">"!!!!!!!!!!!!!!!!after Bench"</span>)</span><br><span class="line">  afterBenchmark(sqlContext.sparkContext)</span><br><span class="line">  result</span><br></pre></td></tr></table></figure></p>
<p>这个善后处理是afterBenchmark方法，方法代码：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">afterBenchmark</span><span class="params">(sc: SparkContext)</span>: Unit </span>= &#123;</span><br><span class="line">  <span class="comment">// Best-effort clean up of weakly referenced RDDs, shuffles, and broadcasts</span></span><br><span class="line">  System.gc()</span><br><span class="line">  <span class="comment">// Remove any leftover blocks that still exist</span></span><br><span class="line">  sc.getExecutorStorageStatus</span><br><span class="line">      .flatMap &#123; status =&gt; status.blocks.map &#123; <span class="keyword">case</span> (bid, _) =&gt; bid &#125; &#125;</span><br><span class="line">      .foreach &#123; bid =&gt; SparkEnv.get.blockManager.master.removeBlock(bid) &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>它会对每个相关block的id进行移除，无论它是否做了cache。所以解决方法是注释掉afterBenchmark方法，与上次堆外内存的bug一样，发现bug的过程是痛苦的，解决方案是简单到发指的,让人痛苦,又让人快乐.</p>
<p>所以说，别人的框架不是万能的，虽然你操作起来更便捷，但你必须遵守它制定的那一些规则，有些规则是操蛋的，当你使用它时，如果一直被操蛋的bug所围困，就需要看看它的源码看看是否有问题，或者与你的策略存在偶然性的冲突。</p>
<p><strong>最后, 我的梦想是成为规则的制定者.</strong></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="大菠萝的任务">大菠萝的任务</h2><p>大菠萝全名diablo technology,是一家硬件技术外企.但是我是一名纯软界的小小码农,怎么会帮一个硬件公司做事呢.起初是这家公司中国区的技术人员跟我们组里的陆博大大比较熟,他们想招一两个懂Spark的人帮他们做]]>
    </summary>
    
      <category term="C语言" scheme="http://GoodGodY.github.io/tags/C%E8%AF%AD%E8%A8%80/"/>
    
      <category term="bug" scheme="http://GoodGodY.github.io/tags/bug/"/>
    
      <category term="内存" scheme="http://GoodGodY.github.io/tags/%E5%86%85%E5%AD%98/"/>
    
  </entry>
  
</feed>
